{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Final Prediction <a class=\"anchor\"  id=\"chapter11\"></a> \n","\n","I will use the BERT model with uncleaned data to predict the test set and submit my final prediction, as it performs just slightly better than RoBERTa on uncleaned data. "]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:52:32.628664Z","iopub.status.busy":"2023-04-09T19:52:32.627876Z","iopub.status.idle":"2023-04-09T19:52:32.640336Z","shell.execute_reply":"2023-04-09T19:52:32.639109Z","shell.execute_reply.started":"2023-04-09T19:52:32.628622Z"},"trusted":true},"outputs":[],"source":["def preprocess_final(df, text_column, target_column, model_name, max_length=50, device='cuda'):\n","    train_cleaned = df[df[target_column].notna()]\n","    test_cleaned = df[df[target_column].isna()]\n","\n","    train_dataset = train_cleaned[[text_column, target_column]]\n","    text = train_dataset[text_column].values\n","    labels = train_dataset[target_column].values\n","\n","    test_dataset = test_cleaned[text_column].values\n","\n","    # Set the device\n","    device = torch.device(device)\n","\n","    # Load the BERT tokenizer\n","    tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n","\n","    # Tokenize the training texts\n","    encoded_dict = tokenizer(text=text.tolist(),\n","                                add_special_tokens=True,\n","                                max_length=max_length,\n","                                truncation=True,\n","                                padding=True, \n","                                return_token_type_ids = False,\n","                                return_attention_mask = True,\n","                                verbose = True)\n","\n","    # Tokenize the test texts\n","    encoded_test = tokenizer(text=test_dataset.tolist(),\n","                                add_special_tokens=True,\n","                                max_length=max_length,\n","                                truncation=True,\n","                                padding=True, \n","                                return_token_type_ids = False,\n","                                return_attention_mask = True,\n","                                verbose = True)\n","\n","    # Convert the TensorFlow tensors to PyTorch tensors\n","    input_ids_train = torch.tensor(encoded_dict['input_ids'])\n","    attention_mask_train = torch.tensor(encoded_dict['attention_mask'])\n","    labels = torch.tensor(labels)\n","    labels = labels.to(torch.int64)\n","\n","    # Test set\n","    input_ids_test = torch.tensor(encoded_test['input_ids'])\n","    attention_mask_test = torch.tensor(encoded_test['attention_mask'])\n","\n","    # Combine the inputs and labels into a TensorDataset\n","    train_dataset = TensorDataset(input_ids_train, attention_mask_train, labels)\n","    test_dataset = TensorDataset(input_ids_test, attention_mask_test)\n","\n","    # Define the dataloaders for the training and test sets\n","    batch_size = 16\n","    train_dataloader = DataLoader(train_dataset, \n","                                  batch_size=batch_size, \n","                                  shuffle=True)\n","    test_dataloader = DataLoader(test_dataset, \n","                                batch_size=batch_size)\n","\n","    return train_dataloader, test_dataloader, train_dataset, test_dataset\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:52:32.647202Z","iopub.status.busy":"2023-04-09T19:52:32.646835Z","iopub.status.idle":"2023-04-09T19:53:50.005972Z","shell.execute_reply":"2023-04-09T19:53:50.004838Z","shell.execute_reply.started":"2023-04-09T19:52:32.647174Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc9422abefa641c8956d6f82ab445301","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a366046b01e549a6b624faf4c62169d3","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  target\n","0   0       1\n","1   2       1\n","2   3       1\n","3   9       1\n","4  11       1"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train_dataloader, test_dataloader, train_dataset, test_dataset = preprocess_final(df, 'text', 'target', 'bert-base-uncased')\n","\n","predictions = train_validate_test('bert-base-uncased', train_dataset, test_dataset, epochs=2, validation=False)\n","\n","# Convert the list of predictions to a numpy array and flatten it\n","predictions = np.concatenate(predictions).flatten()\n","\n","# Create a pandas dataframe with the test ids and the predicted targets\n","submission_df = pd.DataFrame({'id': test_cleaned['id'], 'target': predictions})\n","\n","# Output the dataframe\n","submission_df.head()"]},{"cell_type":"code","execution_count":29,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-04-09T19:53:50.008370Z","iopub.status.busy":"2023-04-09T19:53:50.007634Z","iopub.status.idle":"2023-04-09T19:53:50.035013Z","shell.execute_reply":"2023-04-09T19:53:50.034037Z","shell.execute_reply.started":"2023-04-09T19:53:50.008330Z"},"trusted":true},"outputs":[],"source":["submission_df.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
