{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Model Comparison <a class=\"anchor\"  id=\"chapter10\"></a> \n","\n","I have now trained five neural networks to predict Disaster/Non-Disaster tweets:\n","* BERT with uncleaned data\n","* BERT with cleaned data\n","* RoBERTa with uncleaned data\n","* RoBERTa with cleaned data\n","* Simple FNN with metadata\n","\n","Among the models evaluated, the two that consistently have the highest accuracy, F1 score, precision, and recall, are BERT and RoBERTa on uncleaned text. It's noteworthy that both models seem to perform better on uncleaned text, although the effect is pretty minimal. All four of the BERT and RoBERTa models perform at or above 80% consistently.\n","\n","It's possible that these models perform better on uncleaned data because they are better able to capture patterns from metadata such as punctuation, hashtags, and mentions that were removed during data cleaning. The predictive power of metadata is further evident in the FNN metadata model, which includes features such as tweet word count, number of hashtags, number of mentions, and number of spelling errors, but does not include the text of the tweets itself. Despite this limitation, the FNN model achieves an accuracy of up to 70% on some epochs, which is not as high as the BERT and RoBERTa models' accuracies of around 85%, but still notable considering it only relies on metadata."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
